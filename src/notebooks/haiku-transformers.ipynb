{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the most abstract level, our Haiku Transformer implementation will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(hk.Module):\n",
    "\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(name=\"Transformer\")\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, token_ids):\n",
    "        x = Embedding(config)(token_ids)\n",
    "        for layer_num, layer in enumerate(range(config.n_layers)):\n",
    "            x = TransformerBlock(config, layer_num=layer_num)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a comment on how Haiku modules are typically structured.  The `__init__` function is where we manage module configuration *- storing values like the number of layers in our transformer, our hidden dimension size, and other parameters. In order for Haiku to function properly, the `__init__()` of all `hk.Modules` must call `super().__init__()`, optionally with a `name`.\n",
    "\n",
    "Note that if we wanted we could have moved some of this logic around, constructing our `TokenEmbedding` object in the `__init__` method for instance. Although nothing prevents us from doing this, there are times when we don't have enough information to construct all our modules in the `__init__` method, often because our configuration is dependent on some property of our input (like the max length of the sequence, or the batch size, or whether dropout should be applied).  Because of this, lower level `hk.Modules` are often constructed in the same method that is called when the module is applied to an input (in our case, `__call__`). It's also simply convenient to have the full declaration of module settings in line with the application of the module to an input, to prevent hopping back and forth between methods when reading code.\n",
    "\n",
    "In our example above, we use the `__call__` method as our application method for brevity but we could equivalently use `forward` or any other method name. An `hk.Module` can also expose more than one application method if desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlternateTransformer(hk.Module):\n",
    "    \"\"\"\n",
    "    An equally valid implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(name=\"Transformer\")\n",
    "        self.config = config\n",
    "        self.embedder = Embedding(config)\n",
    "    \n",
    "    def embed(self, tokens):\n",
    "        return self.embedder(tokens)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.embedder(tokens)\n",
    "        for layer in config.n_layers:\n",
    "            x = TransformerBlock(config)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had already implemented the `Embedding` and `TransformerBlock` modules, we could convert our new `hk.Module` to a **JAX** compatible function through the use of `hk.transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll fill out our config later\n",
    "config = {'max_length': 512}\n",
    "\n",
    "def features(tokens):\n",
    "    transformer = Transformer(config)\n",
    "    return transformer(tokens)\n",
    "\n",
    "features_fn = hk.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat!  Let's write our `Embedding` module and plug in some pre-trained model weights so we can start executing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings and Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(hk.Module):\n",
    "    \"\"\"\n",
    "    Embeds tokens and positions into an array of shape:\n",
    "    [n_batch, n_seq, n_hidden]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def __call__(self, token_ids, training=False):\n",
    "        \"\"\"\n",
    "        token_ids: ints of shape (batch, n_seq)\n",
    "        \"\"\"\n",
    "        word_embeddings = self.config['pretrained'][\n",
    "            'embeddings/word_embeddings'\n",
    "        ]\n",
    "        \n",
    "        # We have to flatten our tokens before passing them to the    \n",
    "        # hk.Embed module, as arrays with more than one dimension\n",
    "        # are interpreted as multi-dimensional indexes\n",
    "        flat_token_ids = jnp.reshape(\n",
    "        \ttoken_ids, [token_ids.shape[0] * token_ids.shape[1]]\n",
    "        )\n",
    "        flat_token_embeddings = hk.Embed(\n",
    "            vocab_size=word_embeddings.shape[0],\n",
    "            embed_dim=word_embeddings.shape[1], \n",
    "            # Here we're using hk.initializers.Constant to supply \n",
    "            # pre-trained embeddings to our hk.Embed module\n",
    "            w_init=hk.initializers.Constant(\n",
    "                self.config['pretrained']['embeddings/word_embeddings']\n",
    "            )\n",
    "        )(flat_token_ids)\n",
    "        \n",
    "        # After we've embedded our token IDs, \n",
    "        # we reshape to recover our batch dimension\n",
    "        token_embeddings = jnp.reshape(\n",
    "            flat_token_embeddings, \n",
    "            [\n",
    "                token_ids.shape[0], \n",
    "                token_ids.shape[1], \n",
    "                word_embeddings.shape[1]\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Combine our token embeddings with \n",
    "        # a set of learned positional embeddings\n",
    "        embeddings = (\n",
    "            token_embeddings + PositionEmbeddings(self.config)()\n",
    "        )\n",
    "        embeddings = hk.LayerNorm(\n",
    "            axis=-1, \n",
    "            create_scale=True,\n",
    "            create_offset=True,\n",
    "            # The layer norm parameters are also pretrained,\n",
    "            # so we have to take care to use a constant initializer \n",
    "            # for these as well\n",
    "            scale_init=hk.initializers.Constant(\n",
    "                self.config['pretrained']['embeddings/LayerNorm/gamma']\n",
    "            ),\n",
    "            offset_init=hk.initializers.Constant(\n",
    "                self.config['pretrained']['embeddings/LayerNorm/beta']\n",
    "            )\n",
    "        )(embeddings)\n",
    "        \n",
    "        # Dropout is will be applied later when we finetune our \n",
    "        # Roberta implementation to solve a classification task. \n",
    "        # For now we'll set `training` to False.\n",
    "        if training:\n",
    "            embeddings = hk.dropout(\n",
    "                # Haiku magic: \n",
    "                # We'll explicitly provide a RNG key to haiku later\n",
    "                # to make this function\n",
    "                hk.next_rng_key(), \n",
    "                rate=self.config['embed_dropout_rate'], \n",
    "                x=embeddings\n",
    "            )\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it might appear a bit verbose thanks to a half-dozen comments and my foolhardy attempt to keep text from wrapping in the blog's code widget, don't let your eyes glaze over yet. Our Embedding module is relatively straightforward underneath the hood:\n",
    "\n",
    "* Using `hk.Embed`, we perform a lookup to get the vector that corresponds to each of our token IDs.\n",
    "* We initialize this to the pre-trained embedding matrix we downloaded.\n",
    "* We add `PositionEmbeddings()` -- a `hk.Module` we have yet to define.  Assuming a fixed sequence length, this is nothing more than a static matrix that we broadcast to each sequence in our batch.\n",
    "* We normalize our embeddings using layer norm, applying the scales and offsets learned during pre*training.\n",
    "* Finally, we optionally apply dropout to our embeddings at train time.\n",
    "* Note the `hk.next_rng_key()` feature. With vanilla **JAX**, we would have to make sure to pass our `PRNGKey` around to every module that needs it.  A handy feature of Haiku is that your `PRNGKey` is exposed via the `hk.next_rng_key` utility with the context of `hk.transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddings(hk.Module):\n",
    "    \"\"\"\n",
    "    A position embedding of shape [n_seq, n_hidden]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # For unknown reasons the Roberta position embeddings are \n",
    "        # offset in the position embedding matrix\n",
    "        self.offset = 2\n",
    "\n",
    "    def __call__(self):\n",
    "        pretrained_position_embedding = self.config['pretrained'][\n",
    "            'embeddings/position_embeddings'\n",
    "        ]\n",
    "        position_weights = hk.get_parameter(\n",
    "            \"position_embeddings\", \n",
    "            pretrained_position_embedding.shape,\n",
    "            init=hk.initializers.Constant(pretrained_position_embedding)\n",
    "        )\n",
    "        start = self.offset\n",
    "        end = self.offset + self.config['max_length']\n",
    "        return position_weights[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hk.get_parameter` function is how Haiku keeps track of parameter state for us. The module we're in and the `name` argument passed to `hk.get_parameter` serve as keys to register this new parameter in a Haiku-managed parameter store. If we were writing something similar with vanilla **JAX**, we would have to keep track of these parameters manually. Later on we'll see how `hk.transform` allows us to retrieve the values of our parameters using `init()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Weights and Tokenization\n",
    "Now that you're familiar with `hk.get_parameter`, let's load in some pre-trained model weights so we can test out what we've put together so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from functools import lru_cache\n",
    "\n",
    "import joblib\n",
    "import requests\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# We'll use these later as a means to check our implementation\n",
    "huggingface_roberta = RobertaModel.from_pretrained(\n",
    "\t'roberta-base', output_hidden_states=True\n",
    ")\n",
    "huggingface_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Some light postprocessing to make parameter keys more concise\n",
    "def postprocess_key(key):\n",
    "    key = key.replace('model/featurizer/bert/', '')\n",
    "    key = key.replace(':0', '')\n",
    "    key = key.replace('self/', '')\n",
    "    return key\n",
    "\n",
    "\n",
    "# Cache the downloaded file to go easy on the tubes \n",
    "@lru_cache()\n",
    "def get_pretrained_weights():\n",
    "    # We'll use the weight dictionary from the Roberta encoder at \n",
    "    # https://github.com/IndicoDataSolutions/finetune\n",
    "    remote_url = \"https://bendropbox.s3.amazonaws.com/roberta/roberta-model-sm-v2.jl\"\n",
    "    weights = joblib.load(BytesIO(requests.get(remote_url).content))\n",
    "\n",
    "    weights = {\n",
    "        postprocess_key(key): value\n",
    "        for key, value in weights.items()\n",
    "    }\n",
    "    \n",
    "    # We use huggingface's word embedding matrix because their token ID \n",
    "    # mapping varies slightly from the format in the joblib file above\n",
    "    input_embeddings = huggingface_roberta.get_input_embeddings()\n",
    "    weights['embeddings/word_embeddings'] = (\n",
    "        input_embeddings.weight.detach().numpy()\n",
    "    )\n",
    "    return weights\n",
    "\n",
    "\n",
    "class Scope(object):\n",
    "    \"\"\"\n",
    "    A tiny utility to help make looking up into our dictionary cleaner.\n",
    "    There's no haiku magic here.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights, prefix):\n",
    "        self.weights = weights\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.weights[self.prefix + key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = get_pretrained_weights()\n",
    "print([k for k in pretrained.keys() if 'embedding' in k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have the weights we need for our `Embedding` module, but we're still missing a way to go from text to numeric token IDs in our word embedding matrix.  Let's load in the pre-written tokenizer from huggingface to save ourselves some pain, because the only thing more painful than writing tokenizer code is reading about someone writing tokenizer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "This was a lot less painful than re-implementing a tokenizer\n",
    "\"\"\"\n",
    "encoded = huggingface_tokenizer.batch_encode_plus(\n",
    "    [sample_text, sample_text],\n",
    "    pad_to_max_length=True,\n",
    "    max_length=config['max_length']\n",
    ")\n",
    "sample_tokens = encoded['input_ids']\n",
    "print(sample_tokens[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!  We've passed the `pad_to_max_length` and `max_length` arguments so that the `huggingface_tokenizer` can handle padding out sequences to a constant length for us -- and it's working as the trailing 1's above show us. Token ID 1 corresponds to the padding token used by RoBERTa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our Embedding Module\n",
    "Now we have all the necessary ingredients to test out our embedding layer, let's `hk.transform` the embedding operation and take things for a spin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from jax.random import PRNGKey\n",
    "import numpy as np\n",
    "\n",
    "config = {\n",
    "    'pretrained': pretrained,\n",
    "    'max_length': 512,\n",
    "    'embed_dropout_rate': 0.1\n",
    "}\n",
    "\n",
    "def embed_fn(tokens, training=False):\n",
    "    embedding = Embedding(config)(tokens)\n",
    "    return embedding\n",
    "\n",
    "rng = PRNGKey(42)\n",
    "embed = hk.transform(embed_fn, apply_rng=True)\n",
    "sample_tokens = np.asarray(sample_tokens)\n",
    "params = embed.init(rng, sample_tokens, training=False)\n",
    "embedded_tokens = embed.apply(params, rng, sample_tokens)\n",
    "print(embedded_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we've successfully executed our first snippet of code using Haiku!\n",
    "\n",
    "In the code above, we first wrote `embed_fn` to instantiate an instance of our `Embedding` module and call it.  This is necessary to wrap up all the requisite state for the embedding function for haiku.\n",
    "\n",
    "If you accidentally try to instantiate a `hk.Module` outside of a `hk.transform` context you'll receive a helpful reminder that this isn't permitted. Haiku imposes this constraint so it can \"purify\" our stateful `hk.Module` and convert it into a pure function that functions with **JAX**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = Embedding(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = embed.init(rng, sample_tokens, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `init` method is a haiku utility that gathers up all the parameters of our custom haiku modules for us and consolidates them into a `frozendict`. The `init()` method is also responsible for initializing any unitialized parameters, which is why we pass a source of randomness (our `rng`) as the first argument.\n",
    "\n",
    "Let's inspect our `params` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({key: type(value) for key, value in params.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use a `hk.Module` within another `hk.Module`, it gets placed in a subdictionary.  But if we drill down into our `frozendict` we'll eventually hit the bottom and uncover an `np.ndarray` -- the current state of the weights of our model.  If you're familiar with tensorflow's concept of a variable scope, this should feel familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({key: type(value) for key, value in params['embedding/layer_norm'].items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting, however, that we need to use haiku's `hk.get_parameter` construct (or a `hk.Module`) for the parameters to be tracked automatically.  If we try to use a simple `jnp.ndarray` within the context of `hk.transform` it won't be tracked as a trainable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "def linear_fn(x):\n",
    "    w = jax.random.normal(hk.next_rng_key(), (10, 10))\n",
    "    b = jnp.zeros(10)\n",
    "    return np.dot(w, x) + b\n",
    "\n",
    "linear = hk.transform(linear_fn)\n",
    "params = linear.init(PRNGKey(0), np.random.rand(10))\n",
    "print(params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second new method is `.apply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tokens = embed.apply(params, rng, sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply()` method injects the parameters of our model (and if we've passed `apply_rng` to `hk.transform`, our pseudo-random number generator) to the embed function so that we have all the necessary state to compute the output of our function.  This pattern is how `hk.transform` is able to turn a stateful `hk.Module` class into a **JAX** compatible operation -- `init()` and `apply()` are natural counterparts. Our `init()` method extracts the problematic state from our module, and the second passes that state back into the pure `apply()` method.  Because `apply()` is functionally pure, we're totally free to compose it with any of the standard **JAX** operations like `jit()` and `grad()`!\n",
    "\n",
    "```embedded_tokens = jit(embed.apply)(params, rng, sample_tokens)```\n",
    "\n",
    "In all honestly, that's about all there is to working with Haiku!  That's part of the beauty of it -- Haiku aims to be a library, not a framework -- providing a suite of utilities that complement JAX but doing it's best not to get in your way by requiring custom formats or imposing problematic constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Transformer Block Module\n",
    "\n",
    "With the critical pieces of the haiku API behind us, let's continue implementing our transformer!  Next up -- the `TransformerBlock` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(hk.Module):\n",
    "\n",
    "    def __init__(self, config, layer_num):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n = layer_num\n",
    "\n",
    "    def __call__(self, x, mask, training=False):\n",
    "        scope = Scope(\n",
    "            self.config['pretrained'], f'encoder/layer_{self.n}/'\n",
    "        )\n",
    "        # Feed our input through a multi-head attention operation\n",
    "        attention_output = MultiHeadAttention(\n",
    "            self.config, self.n\n",
    "        )(x, mask, training=training)\n",
    "\n",
    "        # Add a residual connection with the input to the layer\n",
    "        residual = attention_output + x\n",
    "\n",
    "        # Apply layer norm to the combined output\n",
    "        attention_output = hk.LayerNorm(\n",
    "            axis=-1,\n",
    "            create_scale=True,\n",
    "            create_offset=True,\n",
    "            scale_init=hk.initializers.Constant(\n",
    "                scope['attention/output/LayerNorm/gamma']\n",
    "            ),\n",
    "            offset_init=hk.initializers.Constant(\n",
    "                scope['attention/output/LayerNorm/beta']\n",
    "            ),\n",
    "        )(residual)\n",
    "\n",
    "        # Project out to a larger dim, apply a gelu, \n",
    "        # and then project back down to our hidden dim\n",
    "        mlp_output = TransformerMLP(\n",
    "            self.config, self.n\n",
    "        )(attention_output, training=training)\n",
    "\n",
    "        # Residual connection to the output of the attention operation\n",
    "        output_residual = mlp_output + attention_output\n",
    "\n",
    "        # Apply another LayerNorm\n",
    "        layer_output = hk.LayerNorm(\n",
    "            axis=-1,\n",
    "            create_scale=True,\n",
    "            create_offset=True,\n",
    "            scale_init=hk.initializers.Constant(\n",
    "                scope['output/LayerNorm/gamma']\n",
    "            ),\n",
    "            offset_init=hk.initializers.Constant(\n",
    "                scope['output/LayerNorm/beta']\n",
    "            ),\n",
    "        )(output_residual) \n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this level of abstraction the flow is still fairly simple. We feed the inputs to each transformer block through it's signature self-attention layer, then add in residuals and apply layer normalization.\n",
    "We then feed the attention outputs through a 2 layer MLP, apply the residuals from the self-attention output, and apply a second layer normalization step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention\n",
    "\n",
    "Let's define our `MultiHeadAttention` layer next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(hk.Module):\n",
    "\n",
    "    def __init__(self, config, layer_num):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n = layer_num\n",
    "\n",
    "    def _split_into_heads(self, x):\n",
    "        return jnp.reshape(\n",
    "            x, \n",
    "            [\n",
    "                x.shape[0],\n",
    "                x.shape[1],\n",
    "                self.config['n_heads'],\n",
    "                x.shape[2] // self.config['n_heads']\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, mask, training=False):\n",
    "        \"\"\"\n",
    "        x: tensor of shape (batch, seq, n_hidden)\n",
    "        mask: tensor of shape (batch, seq)\n",
    "        \"\"\"\n",
    "        scope = Scope(self.config['pretrained'], f'encoder/layer_{self.n}/attention/')\n",
    "        \n",
    "        # Project to queries, keys, and values\n",
    "        # Shapes are all [batch, sequence_length, hidden_size]\n",
    "        queries = hk.Linear(\n",
    "            output_size=self.config['hidden_size'],\n",
    "            w_init=hk.initializers.Constant(scope['query/kernel']),\n",
    "            b_init=hk.initializers.Constant(scope['query/bias'])\n",
    "        )(x)\n",
    "        keys = hk.Linear(\n",
    "            output_size=self.config['hidden_size'],\n",
    "            w_init=hk.initializers.Constant(scope['key/kernel']),\n",
    "            b_init=hk.initializers.Constant(scope['key/bias'])\n",
    "        )(x)\n",
    "        values = hk.Linear(\n",
    "            output_size=self.config['hidden_size'],\n",
    "            w_init=hk.initializers.Constant(scope['value/kernel']),\n",
    "            b_init=hk.initializers.Constant(scope['value/bias'])\n",
    "        )(x)\n",
    "        \n",
    "        # Reshape our hidden state to group into heads\n",
    "        # New shapes are:\n",
    "        # [batch, sequence_length, n_heads, size_per_head]\n",
    "        queries = self._split_into_heads(queries)\n",
    "        keys = self._split_into_heads(keys)\n",
    "        values = self._split_into_heads(values)\n",
    "        \n",
    "\n",
    "        # Compute per head attention weights \n",
    "        # b: batch\n",
    "        # s: source sequence\n",
    "        # t: target sequence\n",
    "        # n: number of heads\n",
    "        # h: per-head hidden state\n",
    "        \n",
    "        # Note -- we could also write this with jnp.reshape \n",
    "        # and jnp.matmul, but I'm becoming a fan of how concise \n",
    "        # opting to use einsum notation for this kind of operation is.\n",
    "        # For more info, see: \n",
    "        #   https://rockt.github.io/2018/04/30/einsum or \n",
    "        #   any of Noam Shazeer's recent Transformer papers\n",
    "        attention_logits = jnp.einsum('bsnh,btnh->bnst', queries, keys)\n",
    "        attention_logits /= np.sqrt(queries.shape[-1])\n",
    "        # Add logits of mask tokens with a large negative number\n",
    "        # to prevent attending to those terms.\n",
    "        attention_logits += jnp.reshape(\n",
    "        \tmask * -2**32, [mask.shape[0], 1, 1, mask.shape[1]]\n",
    "        )\n",
    "        attention_weights = jax.nn.softmax(attention_logits, axis=-1)\n",
    "        per_head_attention_output = jnp.einsum(\n",
    "            'btnh,bnst->bsnh', values, attention_weights\n",
    "        )\n",
    "        attention_output = jnp.reshape(\n",
    "            per_head_attention_output, \n",
    "            [\n",
    "                per_head_attention_output.shape[0],\n",
    "                per_head_attention_output.shape[1],\n",
    "                per_head_attention_output.shape[2] * \n",
    "                per_head_attention_output.shape[3]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Apply dense layer to output of attention operation\n",
    "        attention_output = hk.Linear(\n",
    "            output_size=self.config['hidden_size'],\n",
    "            w_init=hk.initializers.Constant(\n",
    "                scope['output/dense/kernel']\n",
    "            ),\n",
    "            b_init=hk.initializers.Constant(\n",
    "                scope['output/dense/bias']\n",
    "            )\n",
    "        )(attention_output)\n",
    "\n",
    "        # Apply dropout at training time\n",
    "        if training:\n",
    "            attention_output = hk.dropout(\n",
    "                rng=hk.next_rng_key(),\n",
    "                rate=self.config['attention_drop_rate'],\n",
    "                x=attention_output\n",
    "            )\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We project our hidden state out to key, query, and value tensors of the same dimensions as the input hidden state\n",
    "* We then reshape the last dimension of our matrix to group neighboring activations into N heads\n",
    "* Queries and keys are dotted to produce a measure of agreement that we'll use as an attention logit\n",
    "* We divide by the sequence length to soften our attention distribution\n",
    "* We apply our softmax to produce our attention weights that sum to 1.\n",
    "* We then use our attention weights in conjunction with our values to produce our new hidden state\n",
    "* We reshape our matrices to re-combine the heads.\n",
    "* Finally, we apply a linear projection to our attention outputs and optionally apply dropout at training time.\n",
    "\n",
    "We won't spent too much time on the details here, as the intent of this blog post is to highlight using JAX and Haiku rather than devote too much time to an explanation of self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    We use this in place of jax.nn.relu because the approximation used \n",
    "    produces a non-trivial difference in the output state\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + jax.scipy.special.erf(x / jnp.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerMLP(hk.Module):\n",
    "\n",
    "    def __init__(self, config, layer_num):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n = layer_num\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        # Project out to higher dim\n",
    "        scope = Scope(\n",
    "            self.config['pretrained'], f'encoder/layer_{self.n}/'\n",
    "        )\n",
    "        intermediate_output = hk.Linear(\n",
    "            output_size=self.config['intermediate_size'],\n",
    "            w_init=hk.initializers.Constant(\n",
    "                scope['intermediate/dense/kernel']\n",
    "            ),\n",
    "            b_init=hk.initializers.Constant(\n",
    "                scope['intermediate/dense/bias']\n",
    "            )\n",
    "        )(x)\n",
    "\n",
    "        # Apply gelu nonlinearity\n",
    "        intermediate_output = gelu(intermediate_output)\n",
    "\n",
    "        # Project back down to hidden size\n",
    "        output = hk.Linear(\n",
    "            output_size=self.config['hidden_size'],\n",
    "            w_init=hk.initializers.Constant(\n",
    "                scope['output/dense/kernel']\n",
    "            ),\n",
    "            b_init=hk.initializers.Constant(\n",
    "                scope['output/dense/bias']\n",
    "            ),\n",
    "        )(intermediate_output)\n",
    "\n",
    "        # Apply dropout at training time\n",
    "        if training:\n",
    "            output = hk.dropout(\n",
    "                rng=hk.next_rng_key(), \n",
    "                rate=self.config['fully_connected_drop_rate'],\n",
    "                x=output\n",
    "            )\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final component of our pre-trained RoBERTa model is the Transformer MLP block. The MLP block contains:\n",
    "\n",
    "* A linear up-projection from our hidden size to a larger intermediate hidden representation\n",
    "* The application of a single gaussian error linear unit (GELU)\n",
    "* A linear projection back down to our hidden size\n",
    "\n",
    "As per usual, we optionally apply dropout at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying it All Together – Roberta Featurizer\n",
    "\n",
    "In the code block below, we wrap up all our previous work, embedding our input token IDs with our `Embedding` module and applying 12 layers of our `TransformerBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "\n",
    "class RobertaFeaturizer(hk.Module):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(name=\"Transformer\")\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, token_ids, training=False):\n",
    "        x = Embedding(self.config)(token_ids, training=training)\n",
    "        mask = (token_ids == self.config['mask_id']).astype(jnp.float32)\n",
    "        for layer_num, layer in enumerate(range(config['n_layers'])):\n",
    "            x = TransformerBlock(\n",
    "                config, layer_num=layer_num\n",
    "            )(x, mask, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that final `hk.Module` complete, we'll populate the config object we've been referencing through our `hk.Module`'s and apply `hk.transform` to produce a pure function. Even without attaching a classification head to the pre-trained base, we already have features we could use for purposes of computing textual similarities or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "config = {\n",
    "    'pretrained': pretrained,\n",
    "    'max_length': 512,\n",
    "    'embed_dropout_rate': 0.1,\n",
    "    'fully_connected_drop_rate': 0.1,\n",
    "    'attention_drop_rate': 0.1,\n",
    "    'hidden_size': 768,\n",
    "    'intermediate_size': 3072,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'mask_id': 1,\n",
    "    'weight_stddev': 0.02,\n",
    "\n",
    "    # For use later in finetuning\n",
    "    'n_classes': 2,\n",
    "    'classifier_drop_rate': 0.1,\n",
    "    'learning_rate': 1e-5,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'l2': 0.1,\n",
    "    'n_epochs': 5,\n",
    "    'batch_size': 4\n",
    "}\n",
    "\n",
    "def featurizer_fn(tokens, training=False):\n",
    "    contextual_embeddings = RobertaFeaturizer(config)(\n",
    "        tokens, training=training\n",
    "    )\n",
    "    return contextual_embeddings\n",
    "\n",
    "rng = PRNGKey(42)\n",
    "roberta = hk.transform(featurizer_fn, apply_rng=True)\n",
    "sample_tokens = np.asarray(sample_tokens)\n",
    "params = roberta.init(rng, sample_tokens, training=False)\n",
    "contextual_embedding = jit(roberta.apply)(params, rng, sample_tokens)\n",
    "print(contextual_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks and Debugging JAX\n",
    "\n",
    "Let's check to make sure our implementation matches up with a known functional implementation -- we'll opt to use the hugging face model we instantiated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_token_ids = torch.tensor(huggingface_tokenizer.encode(sample_text)).unsqueeze(0)\n",
    "huggingface_output_state, huggingface_pooled_state, _ = huggingface_roberta.forward(batch_token_ids)\n",
    "print(np.allclose(\n",
    "    huggingface_output_state.detach().numpy(), \n",
    "    contextual_embedding[:1, :batch_token_ids.size()[1]], \n",
    "    atol=1e-3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  The contextual embeddings line up, so our implementation is correct.\n",
    "\n",
    "Admittedly, the first time I ran this things didn't go quite so smoothly -- I missed the subtle difference in the two `gelu` implementations and the difference in outputs was enough to make this check fail.  We can't directly inspect the intermediate outputs of our functions if we have things wrapped in a `jit` call like our example above, but if you remove the `jit` call to `roberta.apply`, we can add vanilla Python print statements to our implementation to track activations at intermediate points in our network and compare to the Hugging Face implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning for Classification\n",
    "\n",
    "Now that our featurizer is implemented, let's wrap this up to use for downstream classification tasks!\n",
    "\n",
    "This is as easy as slicing off the hidden state of the first token and applying a linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClassifier(hk.Module):\n",
    "\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(name=\"Transformer\")\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, token_ids, training=False):\n",
    "        sequence_features = RobertaFeaturizer(self.config)(\n",
    "            token_ids=token_ids, training=training\n",
    "        )\n",
    "        \n",
    "        # Our classifier representation is just the \n",
    "        # output state of our first token\n",
    "        clf_state = sequence_features[:,0,:]\n",
    "        \n",
    "        if training:\n",
    "            clf_state = hk.dropout(\n",
    "                rng=hk.next_rng_key(),\n",
    "                rate=self.config['classifier_drop_rate'],\n",
    "                x=clf_state\n",
    "            )\n",
    "        \n",
    "        # We project down from our hidden dimension \n",
    "        # to n_classes and use this as our softmax logits\n",
    "        clf_logits = hk.Linear(\n",
    "            output_size=self.config['n_classes'],\n",
    "            w_init=hk.initializers.TruncatedNormal(\n",
    "                self.config['weight_stddev']\n",
    "            )\n",
    "        )(clf_state)\n",
    "        \n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Let's plug in a real dataset to try it out.  As much as I dislike the trope of testing text classifiers on sentiment analysis, we'll be using the IMDB Sentiment dataset from tensorflow datasets because it's already packaged up neatly for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def load_dataset(\n",
    "\tsplit, \n",
    "    training, \n",
    "    batch_size, \n",
    "    n_epochs=1, \n",
    "    n_examples=None\n",
    "):\n",
    "    \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
    "    ds = tfds.load(\n",
    "        \"imdb_reviews\", \n",
    "        split=f\"{split}[:{n_examples}]\"\n",
    "    ).cache().repeat(n_epochs)\n",
    "    if training:\n",
    "        ds = ds.shuffle(10 * batch_size, seed=0)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return tfds.as_numpy(ds)\n",
    "\n",
    "n_examples = 25000\n",
    "train = load_dataset(\"train\", training=True, batch_size=4, n_epochs=config['n_epochs'], n_examples=n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add in an `encode_batch` utility to make calling the huggingface tokenizer more concise, transformer our new `RobertaClassifier` module into a pure function, and initialize our model state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(batch_text):\n",
    "    # Accept either utf-8 encoded bytes or unicode\n",
    "    batch_text = [\n",
    "        text.decode('utf-8') if isinstance(text, bytes) else text \n",
    "        for text in batch_text\n",
    "    ]\n",
    "    \n",
    "    # Use huggingface's tokenizer to convert \n",
    "    # from raw text to integer token ids\n",
    "    token_ids = huggingface_tokenizer.batch_encode_plus(\n",
    "        batch_text, \n",
    "        pad_to_max_length=True, \n",
    "        max_length=config['max_length'],\n",
    "    )['input_ids']\n",
    "    return np.asarray(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform, Init, Apply, and JIT!\n",
    "\n",
    "We're in the home stretch now.  We wrap up our `RobertaClassifier` in a function so we can purify it with `hk.transform` – again making sure to pass `apply_rng` as we're using dropout – and initialize the parameters of our `RobertaClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import optix\n",
    "\n",
    "def roberta_classification_fn(batch_token_ids, training):\n",
    "    model = RobertaClassifier(config)(\n",
    "        jnp.asarray(batch_token_ids), \n",
    "        training=training\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "# Purify our RobertaClassifier through the use of hk.transform\n",
    "# and initialize our classifier\n",
    "rng = jax.random.PRNGKey(42)\n",
    "roberta_classifier = hk.transform(roberta_classification_fn, apply_rng=True)\n",
    "params = roberta_classifier.init(\n",
    "    rng, \n",
    "    batch_token_ids=encode_batch(['Sample text', 'Sample text']), \n",
    "    training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we jit compile some functions that use our RoBERTa classifier for computing the loss, measuring model accuracy, and computing gradient updates.\n",
    "\n",
    "The first argument to `roberta_classifier.apply` is always our `params`, and since we used `apply_rng` we also have to pass in an `rng` argument.  After the required haiku arguments we can supply the rest of the arguments our transformed `roberta_classifier` function expects.\n",
    "\n",
    "Note that our `update` function calls our `loss` function -- so although we didn't decorate our `loss` function with `@jax.jit` directly we'll still reap the benefits when we call `update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, rng, batch_token_ids, batch_labels):\n",
    "    logits = roberta_classifier.apply(\n",
    "        params, rng, batch_token_ids, training=True\n",
    "    )\n",
    "    labels = hk.one_hot(batch_labels, config['n_classes'])\n",
    "    softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
    "    softmax_xent /= labels.shape[0]\n",
    "    return softmax_xent\n",
    "\n",
    "@jax.jit\n",
    "def accuracy(params, rng, batch_token_ids, batch_labels):\n",
    "    predictions = roberta_classifier.apply(\n",
    "        params, rng, batch_token_ids, training=False\n",
    "    )\n",
    "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch_labels)\n",
    "\n",
    "@jax.jit\n",
    "def update(params, rng, opt_state, batch_token_ids, batch_labels):\n",
    "    batch_loss, grads = jax.value_and_grad(loss)(\n",
    "        params, rng, batch_token_ids, batch_labels\n",
    "    )\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_params = optix.apply_updates(params, updates)\n",
    "    return new_params, opt_state, batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Finetuning transformers for downstream tasks requires a few tricks for reliable performance -- we'll use a linear warmup + decay along with gradient clipping for our optimizers. **JAX** exposes 2 different sets of optimization utilities in `jax.experimental.optimizers` and `jax.experimental.optix` respectively. As these packages are marked as experimental, I'm not sure if both will be a portion of the **JAX** library long term or if the plan is for one to supercede the other.\n",
    "\n",
    "For finetuning RoBERTa we'll be using the latter, as it includes learning rate schedule utilities through `optix.scale_by_schedule` as well as a utility for gradient clipping with `optix.clip_by_global_norm`. We can apply our bag of optimization tricks in combination with a vanilla adam optimizer using `optix.chain` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lr_schedule(warmup_percentage, total_steps):\n",
    "    def lr_schedule(step):\n",
    "        percent_complete = step / total_steps\n",
    "        before_peak = jax.lax.convert_element_type(\n",
    "            (percent_complete <= warmup_percentage),\n",
    "            np.float32\n",
    "        )\n",
    "        scale = (\n",
    "            (\n",
    "            \tbefore_peak * (percent_complete / warmup_percentage) +\n",
    "                (1 - before_peak)\n",
    "            ) * (1 - percent_complete)\n",
    "        )\n",
    "        return scale\n",
    "    return lr_schedule\n",
    "\n",
    "\n",
    "total_steps = config['n_epochs'] * (n_examples // config['batch_size'])\n",
    "lr_schedule = make_lr_schedule(\n",
    "\twarmup_percentage=0.1, total_steps=total_steps\n",
    ")\n",
    "opt = optix.chain(\n",
    "    optix.clip_by_global_norm(config['max_grad_norm']),\n",
    "    optix.adam(learning_rate=config['learning_rate']),\n",
    "    optix.scale_by_schedule(lr_schedule)\n",
    ")\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we throw together one final utility before writing our training loop -- a short convenience function to print how our train and test accuracy change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_current_performance(params, n_examples=None, splits=('train', 'test')):\n",
    "    # Load our training evaluation and test evaluation splits \n",
    "    if 'train' in splits:\n",
    "        train_eval = load_dataset(\"train\", training=False, batch_size=25, n_examples=n_examples)\n",
    "        # Compute mean train accuracy\n",
    "        train_accuracy = np.mean([\n",
    "            accuracy(\n",
    "                params, \n",
    "                rng, \n",
    "                encode_batch(train_eval_batch['text']), \n",
    "                train_eval_batch['label']\n",
    "            )\n",
    "            for train_eval_batch in train_eval\n",
    "        ])\n",
    "        print(f\"\\t Train validation acc: {train_accuracy:.3f}\")\n",
    "    \n",
    "    if 'test' in splits:\n",
    "        test_eval = load_dataset(\"test\", training=False, batch_size=25, n_examples=n_examples)\n",
    "        # Compute mean test accuracy\n",
    "        test_accuracy = np.mean([\n",
    "            accuracy(\n",
    "                params, \n",
    "                rng,\n",
    "                encode_batch(test_eval_batch['text']), \n",
    "                test_eval_batch['label'],\n",
    "            )\n",
    "            for test_eval_batch in test_eval\n",
    "        ])\n",
    "        print(f\"\\t Test validation accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in our training loop, we simply pull batches of examples from our training data iterator and call the update function to modify the state of our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, train_batch in enumerate(train):\n",
    "    if step % 100 == 0:\n",
    "        print(f\"[Step {step}]\")\n",
    "    if step % 1000 == 0 and step != 0:\n",
    "        measure_current_performance(params, n_examples=100)\n",
    "\n",
    "    # Perform adam update\n",
    "    next_batch = next(train)\n",
    "    batch_token_ids = encode_batch(next_batch['text'])\n",
    "    batch_labels = next_batch['label']\n",
    "    params, opt_state, batch_loss = update(\n",
    "        params, rng, opt_state, batch_token_ids, batch_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all is said and done, we achieve a respectable test accuracy of 0.944 on the IMDB review dataset. Not too shabby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_current_performance(params, n_examples=25000, splits='test')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
